Configure button in Directory Opus settings:
   - **Type:** `Standard Function` (not MS-DOS Batch Function)
   - **Command:** `@nodeselect "C:\_chunker\opus\Chunker_Move.bat" {allfilepath}`
   - **Note:** Use `{allfilepath}` (no `$` suffix)
2. Select files in Directory Opus ‚Üí Click Chunker Move button
3. Files are moved to `02_data` for automatic processing

**PowerShell Script:**
- Core script: `C:\_chunker\Chunker_MoveOptimized.ps1`
- Recursively moves files/folders into `02_data`, preserving relative paths
- Writes `<filename>.origin.json` manifest (original_full_path, times, size, sha256, optional hmac)
- Watcher reads the manifest and populates sidecar `origin` (falls back if missing)
- Includes transcript logging to `C:\_chunker\logs\Chunker_Move_*.txt` for debugging

**Troubleshooting:**
- If window closes instantly, check log files in `C:\_chunker\logs\`
- Batch files use absolute path to PowerShell 7 with fallback to standard PowerShell
- See `ALL_FIXES_APPLIED.md` and `DIRECTORY_OPUS_BUTTON_CONFIGURATION.md` for detailed setup

## KB Operations (OneDrive)

**Primary PC**
- Start watcher: `powershell -File scripts/Start-Watcher.ps1`
- Stop watcher: `powershell -File scripts/Stop-Watcher.ps1`
- Smoke test: `powershell -File scripts/Smoke-Test.ps1`
- Health: `powershell -File scripts/KB-Health.ps1`
- Run report: `powershell -File tools/write_run_report.ps1`
- Config check: `npm run kb:cfg:check`
- Analytics snapshot: `npm run kb:analytics` (pass `-- --days 7` for weekly view)
- Toggle dedupe: `npm run kb:cfg:dedupe:on` / `npm run kb:cfg:dedupe:off`
- Toggle incremental updates: `npm run kb:cfg:incr:on` / `npm run kb:cfg:incr:off`
- Consistency check: `npm run kb:consistency`

**Secondary PC**
- Do not start the watcher. - Use `streamlit run gui_app.py` for search and answers. **Notes**
- Only one watcher process should run. - OneDrive folder must be set to Always keep on this device. - Duplicate protection is active through incremental updates and de-dup logic. - To auto-start on login, import `scripts/KB_Watcher_StartOnLogin.xml` in Task Scheduler (Action ‚Üí Import Task) and confirm the action path points to `C:\_chunker`. - After import or any restart, run `npm run kb:health` to verify a single `Running (PID=‚Ä¶)` instance. - Weekly maintenance: import `scripts/KB_Weekly_Dedupe.xml` to schedule Monday 09:00 cleanups (dedupe + run report) or run manually with `npm run kb:report`. ## üîÑ Consolidation (2025-10-29)
- New sidecar flags (config.json):
  - `enable_json_sidecar` (default: true)
  - `enable_block_summary` (default: true)
  - `enable_grok` (default: false)

Sidecar schema (high-level):
- `file`, `processed_at`, `department`, `type`, `output_folder`, `transcript`
- `chunks[]`: filename, path, size, index
- `code_blocks[]` (for .py): type, name, signature, start_line, end_line, docstring

- Older project iterations (e.g., ClaudeExportFixer, chat_log_chunker_v1, chat_watcher) were unified under `C:\_chunker`. - Historical outputs migrated to `C:\_chunker\04_output\<ProjectName>_<timestamp>`. - Legacy artifacts captured once per project (latest snapshot only):
  - Docs ‚Üí `99_doc\legacy\<ProjectName>_<timestamp>`
  - Config ‚Üí `06_config\legacy\<ProjectName>_<timestamp>`
  - Logs ‚Üí `05_logs\legacy\<ProjectName>_<timestamp>`
  - DB/Backups ‚Üí `03_archive\legacy\<ProjectName>_<timestamp>`
- Script backups stored with timestamp prefixes at
  `C:\Users\carucci_r\OneDrive - City of Hackensack\00_dev\backup_scripts\<ProjectName>\`. - Policy: keep only the latest legacy snapshot per project (older snapshots pruned). ## ‚öôÔ∏è Configuration

Edit `config.json` to customize:

### Core Settings
- **File filter modes**: all, patterns, suffix
- **Supported file extensions**: .txt, .md, .json, .csv, .xlsx, .pdf, .py, .docx, .sql, .yaml, .xml, .log
- **Chunk sizes and processing options**: sentence limits, overlap settings
- **Notification settings**: email alerts and summaries

### RAG Settings
- **`rag_enabled`**: Enable/disable RAG functionality
- **`ollama_model`**: Ollama embedding model (default: nomic-embed-text)
- **`faiss_persist_dir`**: FAISS index storage directory
- **`chroma_persist_dir`**: ChromaDB storage directory (optional)

### LangSmith Settings (Optional)
- **`langsmith_api_key`**: Your LangSmith API key
- **`langsmith_project`**: Project name for tracing
- **`tracing_enabled`**: Enable/disable tracing
- **`evaluation_enabled`**: Enable/disable evaluation

### Monitoring Settings
- **`debounce_window`**: File event debouncing time (seconds)
- **`use_ready_signal`**: Wait for `<filename>.ready` markers before processing atomic pushes
- **`failed_dir`**: Directory for failed file processing quarantine
- **`max_workers`**: Maximum parallel processing workers

### Vector Store Settings
- **`batch.size`**: Number of chunks to insert into ChromaDB per batch (default 500)
- **`batch.flush_every`**: Optional flush cadence for very large ingest jobs
- **`batch.mem_soft_limit_mb`**: Soft memory cap for batching helper
- **`search.ef_search`**: Overrides HNSW search ef after each batch to rebalance recall vs. latency

## üîç RAG Usage

### Setup
1. **Install Dependencies**: `python install_rag_dependencies.py`
2. **Install Ollama**: Download from [ollama.ai](https://ollama.ai/)
3. **Pull Model**: `ollama pull nomic-embed-text`
4. **Enable RAG**: Set `"rag_enabled": true` in `config.json`
5. **Start Processing**: `python watcher_splitter.py`

### Search Knowledge Base

#### Interactive Search
```bash
python rag_search.py
```

#### Command Line Search
```bash
# Single query
python rag_search.py --query "How do I fix vlookup errors?" # Batch search
python rag_search.py --batch queries.txt --output results.json

# Different search types
python rag_search.py --query "Excel formulas" --search-type semantic
python rag_search.py --query "vlookup excel" --search-type keyword
```

#### GUI Search
```bash
streamlit run gui_app.py
```
Opens a browser interface for entering queries, browsing results, and viewing knowledge-base statistics. #### Programmatic Search
```python
from ollama_integration import initialize_ollama_rag

# Initialize RAG system
rag = initialize_ollama_rag()

# Search
results = rag.hybrid_search("How do I fix vlookup errors? ", top_k=5)

# Display results
for result in results:
    print(f"Score: {result['score']:.3f}")
    print(f"Content: {result['content'][:100]}...")
    print(f"Source: {result['metadata']['source_file']}")
```

### Example Output

**Interactive Search Session:**
```
RAG Search Interface
==================================================
Commands:
  search <query> - Search the knowledge base
  semantic <query> - Semantic similarity search
  keyword <query> - Keyword-based search
  stats - Show knowledge base statistics
  quit - Exit the interface

RAG> search How do I fix vlookup errors? Search Results for: 'How do I fix vlookup errors?' ==================================================

1. Score: 0.847 (semantic)
   Source: excel_guide.md
   Type: .md
   Content: VLOOKUP is used to find values in a table. Syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup]). Use FALSE for exact matches...
   Keywords: vlookup, excel, formula, table

2. Score: 0.723 (semantic)
   Source: troubleshooting.xlsx
   Type: .xlsx
   Content: Common VLOOKUP errors include #N/A when lookup value not found, #REF when table array is invalid...
   Keywords: vlookup, error, troubleshooting, excel

Search completed in 0.234 seconds
Found 2 results
```

## üìä Evaluation & Testing

### Automated Evaluation
```bash
# Run comprehensive evaluation
python automated_eval.py

# Run specific tests
python rag_test.py

# Generate evaluation report
python -c "from automated_eval import AutomatedEvaluator; evaluator = AutomatedEvaluator({}); evaluator.generate_csv_report()"
```

### Manual Evaluation
```python
from rag_evaluation import RAGEvaluator
from rag_integration import FaithfulnessScorer

# Initialize evaluator
evaluator = RAGEvaluator()

# Evaluate retrieval quality
retrieval_metrics = evaluator.evaluate_retrieval(
    retrieved_docs=["doc1.md", "doc2.xlsx"],
    relevant_docs=["doc1.md", "doc2.xlsx", "doc3.pdf"],
    k_values=[1, 3, 5]
)

# Evaluate generation quality
generation_metrics = evaluator.evaluate_generation(
    reference="Check data types and table references",
    generated="Verify data types and table references for vlookup errors"
)

# Evaluate faithfulness
scorer = FaithfulnessScorer()
faithfulness_score = scorer.calculate_faithfulness(
    answer="VLOOKUP requires exact data types",
    context="VLOOKUP syntax requires exact data type matching"
)

print(f"Precision@5: {retrieval_metrics['precision_at_5']:.3f}")
print(f"ROUGE-1: {generation_metrics['rouge1']:.3f}")
print(f"Faithfulness: {faithfulness_score:.3f}")
```

### LangSmith Integration
```python
from langsmith_integration import initialize_langsmith

# Initialize LangSmith
langsmith = initialize_langsmith(
    api_key="your_api_key",
    project="chunker-rag-eval"
)

# Create evaluation dataset
test_queries = [
    {
        "query": "How do I fix vlookup errors? ",
        "expected_answer": "Check data types and table references",
        "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"]
    }
]

# Run evaluation
results = langsmith.run_evaluation(test_queries, rag_function)
```

## üìÅ Supported File Types

| Type | Extensions | Processing Method | Metadata Extracted |
|------|------------|------------------|-------------------|
| **Text** | .txt, .md, .log | Direct text processing | Word count, sentences, keywords |
| **Structured** | .json, .csv, .yaml, .xml | Parsed structure | Schema, data types, samples |
| **Office** | .xlsx, .xls, .xlsm, .docx | Library extraction via `openpyxl` and friends | Sheets, formulas, tab names, basic workbook metadata |
| **Code** | .py, .m, .dax, .ps1, .sql | AST/regex parsing | Functions, classes, queries, imports, docstrings |
| **Documents** | .pdf | Text extraction via `PyPDF2` | Pages, basic PDF metadata, text content |
| **Simulink** | .slx | ZIP/XML extraction with safety limits | Block names, parameters, comments (from XML/MDL members) |

## üõ†Ô∏è Advanced Features

### Real-time Monitoring
```python
from watchdog_system import create_watchdog_monitor

# Initialize watchdog monitor
monitor = create_watchdog_monitor(config, process_callback)

# Start monitoring
monitor.start()

# Monitor stats
stats = monitor.get_stats()
print(f"Queue size: {stats['queue_size']}")
print(f"Processing files: {stats['processing_files']}")
```

### Modular File Processing
```python
from file_processors import process_excel_file, process_pdf_file

# Process specific file types
excel_content = process_excel_file("", "data.xlsx")
pdf_content = process_pdf_file("", "document.pdf")
```

### Embedding Management
```python
from embedding_helpers import EmbeddingManager

# Initialize embedding manager
manager = EmbeddingManager(chunk_size=1000, chunk_overlap=200)

# Process files for embedding
results = batch_process_files(file_paths, manager, extract_keywords_func)
```

## üöÄ Performance & Scalability

- **Parallel Processing**: Multi-threaded file processing with configurable workers
- **Streaming**: Large file support with memory-efficient streaming
- **Caching**: FAISS index persistence for fast startup
- **Debouncing**: Prevents duplicate processing of rapidly changing files
- **Graceful Degradation**: Continues working even if optional components fail

## üîß Troubleshooting

### Common Issues

1. **ChromaDB Installation Fails (Windows)**
   ```bash
   # Use FAISS instead
   pip install faiss-cpu
   # Or install build tools
   # Or use Docker deployment
   ```

2. **Ollama Not Available**
   ```bash
   # Install Ollama from https://ollama.ai/
   # Pull the model
   ollama pull nomic-embed-text
   ```

3. **Memory Issues with Large Files**
   ```python
   # Enable streaming in config
   "enable_streaming": true,
   "stream_chunk_size": 1048576  # 1MB chunks
   ```

4. **UnicodeEncodeError in PowerShell Logs (Windows)**
   ```powershell
   # Switch console to UTF-8 before starting the watcher
   chcp 65001
   Set-Item env:PYTHONIOENCODING utf-8
   python watcher_splitter.py
   ```
   This prevents logging failures when filenames contain emoji or other non-ASCII characters. ### Performance Optimization

- **Chunk Size**: Adjust based on content type (75 for police, 150 for admin)
- **Parallel Workers**: Set based on CPU cores (default: 4)
- **Debounce Window**: Increase for slow file systems (default: 1s)
- **Index Persistence**: Enable for faster startup after restart

## üìà Monitoring & Analytics

- **Database Tracking**: SQLite database with processing statistics
- **Session Metrics**: Files processed, chunks created, performance metrics
- **Error Logging**: Comprehensive error tracking and notification
- **System Metrics**: CPU, memory, disk usage monitoring
- **RAG Metrics**: Search performance, evaluation scores, user feedback

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details. ## üôè Acknowledgments

- **Ollama** for local embedding models
- **FAISS** for vector similarity search
- **LangChain** for RAG framework
- **LangSmith** for evaluation and tracing
- **Watchdog** for file system monitoring

## üîÑ Version Control & GitHub

### Git Repository
This project is version-controlled using Git and backed up to GitHub. **Remote Repository:** `https://github.com/racmac57/chunker_Web.git`

### Quick Git Commands
```bash
# Check status
git status

# Stage and commit changes
git add -A
git commit -m "Description of changes"

# Push to GitHub
git push origin main

# View commit history
git log --oneline -10
```

### Files Excluded from Git
The following are automatically excluded via `.gitignore`:
- Processed documents (`99_doc/`, `04_output/`)
- Archived files (`03_archive/`)
- Database files (`*.db`, `*.sqlite`)
- Log files (`logs/`, `*.log`)
- Virtual environments (`.venv/`, `venv/`)
- NLTK data (`nltk_data/`)
- Temporary and backup files

### Contributing via Git
1. Clone the repository: `git clone https://github.com/racmac57/chunker_Web.git`
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and commit: `git commit -m "Feature: description"`
4. Push to your fork and create a pull request

For detailed Git setup information, see `GIT_SETUP_STATUS.md`. ## Directory Health

**Last Cleanup:** 2025-10-31 19:22:39  
**Items Scanned:** 16595  
**Items Moved:** 7  
**Items Deleted:** 627  
**Snapshots Pruned:** 0

**Snapshot Policy:** Keep only the latest legacy snapshot per project. Older snapshots are pruned during maintenance. Config backups follow the same policy. **Log Location:** `05_logs/maintenance/2025_10_31_19_16_35/`

**Git Status:** ‚úÖ Repository initialized, connected to GitHub, and regularly backed up
[! [Release](https://img.shields.io/github/v/release/racmac57/chunker_Web?display_name=tag&sort=semver)](https://github.com/racmac57/chunker_Web/releases/latest)
[! [CI](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml/badge.svg)](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml)
[! [CI](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml/badge.svg)](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml)
[! [Release](https://img.shields.io/github/v/release/racmac57/chunker_Web?display_name=tag&sort=semver)](https://github.com/racmac57/chunker_Web/releases/latest)

```

Correcting the CHANGELOG entry for 2025-12-02:


```markdown
# Changelog

All notable changes to the Enterprise Chunker system will be documented in this file. The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html). ## [v2.1.9] - 2025-12-04 - Chunker Move Scripts & PATH Fixes

### Fixed (2025-12-04)
- **Critical PATH Visibility Fix**: Updated `Chunker_Move.bat` and `Chunker_MoveOptimized.bat` to use absolute path to PowerShell 7 (`C:\Program Files\PowerShell\7\pwsh.exe`) instead of relying on system PATH
  - Prevents immediate window closure when `pwsh.exe` is not in PATH
  - Added automatic fallback to standard Windows PowerShell (`powershell.exe`) if PowerShell 7 not found
  - Both Directory Opus and Send To menu batch files now work reliably regardless of PATH configuration
- **Directory Opus Button Configuration**: Documented correct button settings:
  - Type must be `Standard Function` (not `MS-DOS Batch Function`)
  - Argument syntax: `{allfilepath}` (not `{allfilepath$}`)
  - Command includes `@nodeselect` modifier for better UX
- **Enhanced Debug Output**: Added comprehensive debug messages to batch files showing script path, target files, and processing steps
- **Transcript Logging**: Enhanced PowerShell script with `Start-Transcript` to capture all output to log files even if window closes instantly

### Changed (2025-12-04)
- **Batch File Error Handling**: Improved error messages and validation in both Directory Opus and Send To batch files
- **PowerShell Path Resolution**: Both batch files now verify PowerShell executable exists before attempting to use it
- **Window Display**: Enhanced window output to show processing steps and debug information

### Documentation (2025-12-04)
- Created comprehensive troubleshooting guides:
  - `ALL_FIXES_APPLIED.md` - Complete fix summary
  - `DIRECTORY_OPUS_BUTTON_CONFIGURATION.md` - Step-by-step button setup
  - `OPUS_BUTTON_QUICK_REFERENCE.txt` - Quick reference card
  - `FIX_SUMMARY_SENDTO_AND_OPUS.md` - Issue resolution summary
- Updated README.md with Send To menu and Directory Opus integration details
- Updated SUMMARY.md with PATH fix information

---

## [v2.1.9] - 2025-12-02 - Directory Opus Integration & Multicore Processing

### Added (2025-12-02)
- **Directory Opus Button Integration**: Added `C:\_chunker\opus\Chunker_Move.bat` for seamless file processing from Directory Opus
  - Configured as Standard Function button with `{allfilepath}` argument passing (updated to Standard Function in 2025-12-04 fix)
  - Wraps `Chunker_MoveOptimized.ps1` for moving files to watch folder
  - Window display configured for operation visibility
- **Watcher Startup Script Enhancement**: Improved `C:\_chunker\scripts\Start-Watcher.bat`:
  - Silent operation (no window flash) using `-WindowStyle Hidden`
  - Comprehensive error detection with prerequisite validation
  - Automatic process verification after startup
  - Detailed logging to `logs\watcher_start.log` for troubleshooting
  - Full path validation for PowerShell script, Python script, and directories

### Changed (2025-12-02)
- **Multicore Processing Enabled**: Set `use_multiprocessing: true` in `config.json` for true CPU-bound parallel processing
  - Enables full utilization of multiple CPU cores for file processing
  - Expected 100%+ performance improvement similar to laptop performance gains
  - Automatic fallback to sequential processing on errors
- **Database Performance**: Optimized database operations for multiprocessing:
  - Increased `database_batch_size` from 10 to 50 in `config.json` to reduce write frequency
  - Enhanced retry logic in `chunker_db.py` with exponential backoff (1s, 2s, 4s, 5s max)
  - Increased retries from 3 to 5 attempts for better SQLite lock handling
  - Reduced "database is locked" errors during concurrent multiprocessing operations

### Fixed (2025-12-02)
- **Stop-Watcher.ps1 Variable Conflict**: Fixed PowerShell execution error by renaming `$pid` variable to `$watcherPid` to avoid conflict with PowerShell's built-in read-only `$PID` variable

### Documentation (2025-12-02)
- Updated README.md with Directory Opus integration and multicore processing improvements
- Updated SUMMARY.md with watcher script enhancements and database optimizations
- Documented Directory Opus button configuration in CHANGELOG.md

---

## [v2.1.9] - 2025-11-19‚Äì23 - Performance Improvements, Large Backlog Support & Failed File Analysis

### Added (2025-11-23)
- **Expanded File Type Support**: Added `.yaml` and `.docx` to `supported_extensions` in `config.json`, bringing total supported file types from 13 to 15. Processors for these file types already existed in `file_processors.py` but were not enabled in the watcher configuration. ### Changed (2025-11-23)
- **config.json**: Added `.yaml` and `.docx` to `supported_extensions` array to enable processing of YAML configuration files and Word documents. ### Documentation (2025-11-23)
- Updated `README.md` to reflect 15 supported file types and expanded file type processing capabilities. - Updated `SUMMARY.md` with v2.1.9 changes for November 23. ---

### Added (2025-11-19)
- **Failed File Analysis**: New `analyze_failed_files.py` script provides comprehensive analysis of failed files by:
  - File type and extension distribution
  - Size analysis and categorization
  - Time pattern analysis (failure bursts, age distribution)
  - Reprocessing potential assessment (identifies files that might succeed with updated code)
- **OneDrive Failed Directory**: Failed files now default to OneDrive path (`%OneDriveCommercial%\\KB_Shared\\03_archive\\failed`) for consistency with archive and output directories
- **Environment Variable Expansion**: Enhanced `load_cfg()` function to expand environment variables for `failed_dir` configuration

### Changed (2025-11-19)
- **config.json**: Added `"failed_dir": "%OneDriveCommercial%\\KB_Shared\\03_archive\\failed"` to use OneDrive path for failed files
- **watcher_splitter.py**: Updated `load_cfg()` function to expand environment variables for `failed_dir` configuration key

### Documentation (2025-11-19)
- Added `HANDOFF_PROMPT.md` - Comprehensive handoff documentation for AI assistants with project context, current state, and recommendations
- Updated README.md with failed file analysis tools and OneDrive failed directory configuration
- Updated SUMMARY.md with v2.1.9 changes for November 19

---

### Added (2025-11-20)
- **Failed File Tracker**: New `failed_file_tracker.py` module with SQLite backend to track failed files, classify failure types (e.g. `encrypted_pdf`, `corrupt_file`, `invalid_chunk_file`), enforce capped retries with exponential backoff, and expose CLI stats and JSON exports. - **Batch Failed-File Reprocessing**: New `batch_reprocess_failed.py` script that:
  - Categorizes `03_archive/failed` into `retryable`, `permanent_chunk` (Oct 27 incident chunk files), and `permanent_tracker` sets. - Requeues retryable failures into the `source` directory in batches (default 500, supports `--pilot` and `--all`). - Writes detailed JSON stats to `05_logs/batch_reprocess_stats.json`. - **Reprocessing Metrics & Planning**:
  - Enhanced `reprocess_output.py` with per-extension success/fail/skip metrics and JSON reporting (`05_logs/reprocess_stats.json`). - Added `REPROCESSING_RUN_PLAN.md` documenting the end-to-end workflow for reprocessing historical failures and validating OneDrive outputs. - **Enhanced PDF/SLX Support**:
  - `file_processors.py` now adds page markers (e.g. `=== [PAGE 1/10] ===`), extracts basic PDF metadata (title, author, total pages), and handles encrypted PDFs gracefully by returning a clear marker instead of raising. - Improved `.slx` handling in `watcher_splitter.py` with per-file content limits increased from 5 KB to 50 KB and a total 100 MB safety cap for ZIP extraction. ### Changed (2025-11-20)
- **Reprocessing Configuration**: Updated `config.json` to:
  - Confirm `min_file_size_bytes` = 100, aligning code and docs for tiny-file archiving into `03_archive/skipped_files/`. - Enable dedup auto-remove (`deduplication.auto_remove = true`, `log_only = false`) so detected duplicates are actively removed from ChromaDB rather than only logged. - **Watcher Integration**: Integrated `failed_file_tracker` into `watcher_splitter.py` error handling paths so that:
  - Files are checked with `should_retry()` before processing. - `record_success()` and `record_failure()` are called appropriately. - Certain failure types are immediately marked permanent (e.g. `invalid_chunk_file` for Oct 27 chunk artifacts). ### Documentation (2025-11-20)
- Updated `README.md` and `SUMMARY.md` to document:
  - Failed-file analysis tools and OneDrive failed directory behavior. - `failed_file_tracker.py`, `batch_reprocess_failed.py`, and `REPROCESSING_RUN_PLAN.md`. - Enhanced PDF/Excel/SLX processing behavior and its impact on RAG. ---

## [Unreleased] - Desktop Sync & Extended File Types (2025-11-21)

### Added (2025-11-21)
- **Archive Management Scripts**: New utilities for managing archive files across desktop and laptop:
  - `move_archive_to_kb_location.py` - Safely move archive files to KB_Shared location for cross-machine access
  - `archive_file_management.py` - Analyze archive files and provide migration recommendations
  - `find_and_move_archive_outputs.py` - Find and move output folders with duplicate detection
- **Deduplication Utility**: `dedupe_local_vs_kb.py` - Compare local files against KB_Shared to identify and optionally remove duplicates

### Changed (2025-11-21)
- **Log Rotation Error Handling**: Fixed `PermissionError` crash in `setup_logging()` when log file is locked by another process (Windows WinError 32). Log rotation now fails gracefully with a warning instead of crashing the watcher. - **Output Format Enhancement**: Transcript files now created in both `.md` and `.json` formats for better compatibility and structured access:
  - Markdown transcript includes headers, metadata, and formatted content
  - JSON transcript includes structured metadata, chunk indices, and combined content
- **Extended File Type Support**: Added `.yaml` and `.docx` support to `config.json` (bringing total to 15 file types). Files are processed via `file_processors.py` with appropriate extraction methods. - **Timestamp Format Documentation**: Clarified output folder naming format: `YYYY_MM_dd_HH_MM_SS_[File_Name]` with explicit comments in code. ### Documentation (2025-11-21)
- Updated README.md to reflect extended file type support (15 types total)
- Updated SUMMARY.md with desktop sync improvements and archive management
- Documented output format changes (dual transcript files)
- Added cross-machine archive file management guidance

---

## [v2.1.9] - 2025-11-18 - Performance Improvements & Large Backlog Support

### Added
- **Batch Processing**: Configurable batch size (default 100 files per cycle) to prevent system overload on large backlogs
- **Stability Skip Optimization**: Files older than `stability_skip_minutes` (default 10) bypass expensive 2-second stability checks, dramatically reducing processing time
- **Enhanced Parallel Processing**: Optional multiprocessing mode (`use_multiprocessing`) with automatic fallback to sequential processing (`multiprocessing_fallback`)
- **Archive Reprocessing**: New `reprocess_output.py` script enables reprocessing of archived files with enhanced tagging and domain-aware department detection
- **OneDrive Migration**: `migrate_to_onedrive.py` safely migrates local archives to OneDrive with conflict resolution and Windows MAX_PATH handling (240 character limit)
- **Department Configuration Refactoring**: 20 domain-specific departments (python, cad, claude, data-cleaning, excel, arcgis, powerbi, sql, fire, ems, etc.)