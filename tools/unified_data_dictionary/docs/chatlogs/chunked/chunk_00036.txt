",
        "expected_answer": "Check data types and table references",
        "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"]
    }
]

# Run evaluation
results = langsmith.run_evaluation(test_queries, rag_function)
```

## ğŸ“ Supported File Types

| Type | Extensions | Processing Method | Metadata Extracted |
|------|------------|------------------|-------------------|
| **Text** | .txt, .md, .log | Direct text processing | Word count, sentences, keywords |
| **Structured** | .json, .csv, .yaml, .xml | Parsed structure | Schema, data types, samples |
| **Office** | .xlsx, .xls, .xlsm, .docx | Library extraction via `openpyxl` and friends | Sheets, formulas, tab names, basic workbook metadata |
| **Code** | .py, .m, .dax, .ps1, .sql | AST/regex parsing | Functions, classes, queries, imports, docstrings |
| **Documents** | .pdf | Text extraction via `PyPDF2` | Pages, basic PDF metadata, text content |
| **Simulink** | .slx | ZIP/XML extraction with safety limits | Block names, parameters, comments (from XML/MDL members) |

## ğŸ› ï¸ Advanced Features

### Real-time Monitoring
```python
from watchdog_system import create_watchdog_monitor

# Initialize watchdog monitor
monitor = create_watchdog_monitor(config, process_callback)

# Start monitoring
monitor.start()

# Monitor stats
stats = monitor.get_stats()
print(f"Queue size: {stats['queue_size']}")
print(f"Processing files: {stats['processing_files']}")
```

### Modular File Processing
```python
from file_processors import process_excel_file, process_pdf_file

# Process specific file types
excel_content = process_excel_file("", "data.xlsx")
pdf_content = process_pdf_file("", "document.pdf")
```

### Embedding Management
```python
from embedding_helpers import EmbeddingManager

# Initialize embedding manager
manager = EmbeddingManager(chunk_size=1000, chunk_overlap=200)

# Process files for embedding
results = batch_process_files(file_paths, manager, extract_keywords_func)
```

## ğŸš€ Performance & Scalability

- **Parallel Processing**: Multi-threaded file processing with configurable workers
- **Streaming**: Large file support with memory-efficient streaming
- **Caching**: FAISS index persistence for fast startup
- **Debouncing**: Prevents duplicate processing of rapidly changing files
- **Graceful Degradation**: Continues working even if optional components fail

## ğŸ”§ Troubleshooting

### Common Issues

1. **ChromaDB Installation Fails (Windows)**
   ```bash
   # Use FAISS instead
   pip install faiss-cpu
   # Or install build tools
   # Or use Docker deployment
   ```

2. **Ollama Not Available**
   ```bash
   # Install Ollama from https://ollama.ai/
   # Pull the model
   ollama pull nomic-embed-text
   ```

3. **Memory Issues with Large Files**
   ```python
   # Enable streaming in config
   "enable_streaming": true,
   "stream_chunk_size": 1048576  # 1MB chunks
   ```

4. **UnicodeEncodeError in PowerShell Logs (Windows)**
   ```powershell
   # Switch console to UTF-8 before starting the watcher
   chcp 65001
   Set-Item env:PYTHONIOENCODING utf-8
   python watcher_splitter.py
   ```
   This prevents logging failures when filenames contain emoji or other non-ASCII characters. ### Performance Optimization

- **Chunk Size**: Adjust based on content type (75 for police, 150 for admin)
- **Parallel Workers**: Set based on CPU cores (default: 4)
- **Debounce Window**: Increase for slow file systems (default: 1s)
- **Index Persistence**: Enable for faster startup after restart

## ğŸ“ˆ Monitoring & Analytics

- **Database Tracking**: SQLite database with processing statistics
- **Session Metrics**: Files processed, chunks created, performance metrics
- **Error Logging**: Comprehensive error tracking and notification
- **System Metrics**: CPU, memory, disk usage monitoring
- **RAG Metrics**: Search performance, evaluation scores, user feedback

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details. ## ğŸ™ Acknowledgments

- **Ollama** for local embedding models
- **FAISS** for vector similarity search
- **LangChain** for RAG framework
- **LangSmith** for evaluation and tracing
- **Watchdog** for file system monitoring

## ğŸ”„ Version Control & GitHub

### Git Repository
This project is version-controlled using Git and backed up to GitHub. **Remote Repository:** `https://github.com/racmac57/chunker_Web.git`

### Quick Git Commands
```bash
# Check status
git status

# Stage and commit changes
git add -A
git commit -m "Description of changes"

# Push to GitHub
git push origin main

# View commit history
git log --oneline -10
```

### Files Excluded from Git
The following are automatically excluded via `.gitignore`:
- Processed documents (`99_doc/`, `04_output/`)
- Archived files (`03_archive/`)
- Database files (`*.db`, `*.sqlite`)
- Log files (`logs/`, `*.log`)
- Virtual environments (`.venv/`, `venv/`)
- NLTK data (`nltk_data/`)
- Temporary and backup files

### Contributing via Git
1. Clone the repository: `git clone https://github.com/racmac57/chunker_Web.git`
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and commit: `git commit -m "Feature: description"`
4. Push to your fork and create a pull request

For detailed Git setup information, see `GIT_SETUP_STATUS.md`. ## Directory Health

**Last Cleanup:** 2025-10-31 19:22:39  
**Items Scanned:** 16595  
**Items Moved:** 7  
**Items Deleted:** 627  
**Snapshots Pruned:** 0

**Snapshot Policy:** Keep only the latest legacy snapshot per project. Older snapshots are pruned during maintenance. Config backups follow the same policy. **Log Location:** `05_logs/maintenance/2025_10_31_19_16_35/`

**Git Status:** âœ… Repository initialized, connected to GitHub, and regularly backed up
[! [Release](https://img.shields.io/github/v/release/racmac57/chunker_Web?display_name=tag&sort=semver)](https://github.com/racmac57/chunker_Web/releases/latest)
[! [CI](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml/badge.svg)](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml)
[! [CI](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml/badge.svg)](https://github.com/racmac57/chunker_Web/actions/workflows/ci.yml)
[! [Release](https://img.shields.io/github/v/release/racmac57/chunker_Web?display_name=tag&sort=semver)](https://github.com/racmac57/chunker_Web/releases/latest)

```

```markdown
# Chunker_v2 - Enterprise RAG-Powered File Processing System

**Version 2.1.9** - Performance improvements for large backlogs, enhanced reprocessing and failed-file handling, OneDrive migration support, and expanded file type processing (PDF/XLSX/SLX/YAML/DOCX). Now supports 16 file types with desktop/laptop synchronization improvements. ## What's New in v2.1.9+

### Chunker Move Scripts & PATH Fixes (2025-12-04)
- **Critical PATH Fix**: Fixed batch files to use absolute path to PowerShell 7 (`C:\Program Files\PowerShell\7\pwsh.exe`) instead of relying on system PATH
  - Prevents window closing instantly when `pwsh.exe` is not in PATH
  - Added automatic fallback to standard Windows PowerShell if PowerShell 7 not found
  - Both Directory Opus and Send To menu batch files now work reliably regardless of PATH configuration
- **Enhanced Debugging**: Added comprehensive debug output and transcript logging to capture errors even if window closes
- **Directory Opus Button Configuration**: Documented correct settings (Type: `Standard Function`, Argument: `{allfilepath}`, Command includes `@nodeselect` modifier)

### Directory Opus Integration & Multicore Processing (2025-12-02)
- **Directory Opus Button Integration**: Added `C:\_chunker\opus\Chunker_Move.bat` for seamless file processing from Directory Opus file manager
  - Configured as Standard Function button with `{allfilepath}` argument passing
  - Moves selected files directly to `02_data` watch folder for automatic processing
  - Window display configured for visibility during operations
- **Watcher Startup Improvements**: Enhanced `C:\_chunker\scripts\Start-Watcher.bat` with:
  - Silent operation (no window flash) using `-WindowStyle Hidden`
  - Comprehensive error detection and reporting
  - Automatic verification that watcher process actually started
  - Detailed logging to `logs\watcher_start.log` for troubleshooting
  - Full path validation for all prerequisites
- **Stop Watcher Fix**: Fixed `C:\_chunker\scripts\Stop-Watcher.ps1` variable conflict (`$pid` â†’ `$watcherPid`) preventing PowerShell execution errors
- **Multicore Processing Enabled**: Set `use_multiprocessing: true` in `config.json` for true CPU-bound parallel processing across multiple cores
  - Expected 100%+ performance improvement on CPU-intensive workloads
  - Automatic fallback to sequential processing on errors
- **Database Performance Improvements**:
  - Increased `database_batch_size` from 10 to 50 to reduce write frequency
  - Enhanced retry logic in `chunker_db.py` with exponential backoff (1s, 2s, 4s, 5s max)
  - Increased retries from 3 to 5 attempts for better lock handling
  - Reduced "database is locked" errors during multiprocessing operations

### Desktop/Laptop Synchronization & Extended File Types (2025-11-21)
- **Cross-Machine Archive Management**: New utilities for managing files across desktop and laptop:
  - `move_archive_to_kb_location.py` - Move archive files to KB_Shared for cross-machine access
  - `archive_file_management.py` - Analyze and manage archive files with recommendations
  - `find_and_move_archive_outputs.py` - Find output folders with duplicate detection
  - `dedupe_local_vs_kb.py` - Compare local files against KB_Shared to prevent duplicates
- **Log Rotation Resilience**: Fixed `PermissionError` crash when log file is locked (Windows WinError 32). Watcher now handles locked log files gracefully without crashing. - **Dual Transcript Format**: Output folders now include both `.md` and `.json` transcript files for better compatibility and structured access. - **Extended File Types**: Added `.yaml` and `.docx` support, bringing total supported file types to 16. - **Timestamp Format**: Consistent output folder naming: `YYYY_MM_dd_HH_MM_SS_[File_Name]` format documented and standardized. ### Performance Improvements & Reprocessing (2025-11-18â€“20)
- **Batch Processing**: Configurable batch size (default 100 files per cycle) prevents system overload on large backlogs
- **Stability Skip Optimization**: Files older than `stability_skip_minutes` (default 10) bypass expensive stability checks, dramatically reducing processing time for large backlogs
- **Enhanced Parallel Processing**: Optional multiprocessing mode with automatic fallback to sequential processing on errors
- **Archive Reprocessing**: New `reprocess_output.py` script enables reprocessing of archived files with enhanced tagging and department detection
- **OneDrive Migration**: `migrate_to_onedrive.py` safely migrates local archives to OneDrive with conflict resolution and path length checks
- **Department Configuration Refactoring**: Domain-aware department detection (20 specialized departments: python, cad, claude, data-cleaning, fire, ems, etc.) with tailored chunk sizes and settings
- **Auto-Archival**: Optional weekly archival of old output sessions (>90 days) to `03_archive/consolidated/YYYY/MM/`
- **Long Path Handling**: Automatic path shortening for Windows MAX_PATH limits (>240 characters)
- **Version Conflict Resolution**: Automatic `_v2`, `_v3` suffix handling for sidecars and manifests to prevent overwrites
- **Failed File Analysis**: New `analyze_failed_files.py` script provides comprehensive analysis of failed files by type, size, time patterns, and reprocessing potential
- **OneDrive Failed Directory**: Failed files now use OneDrive path by default for consistency with archive and output directories (`failed_dir` in `config.json`)
- **Failed File Tracker**: New `failed_file_tracker.py` module tracks failures with a SQLite backend, classifies failure types, applies capped retries with exponential backoff, and exposes CLI stats/exports
- **Batch Failed-File Reprocessing**: New `batch_reprocess_failed.py` safely requeues retryable failures from `03_archive/failed` into `source/` with categorization (retryable vs permanent chunk vs tracker-permanent) and JSON stats
- **Reprocessing Run Plan**: `REPROCESSING_RUN_PLAN.md` documents the end-to-end plan for reprocessing historical failures and confirming OneDrive outputs
- **Reprocessing Metrics**: `reprocess_output.py` now tracks per-extension success/fail/skip counts and writes JSON reports to `05_logs/reprocess_stats.json`
- **Enhanced PDF Processing**: `file_processors.py` now adds page markers (e.g. `=== [PAGE 1/10] ===`), extracts basic metadata (title, author, total pages), and handles encrypted PDFs gracefully with clear markers instead of hard failures
- **Enhanced SLX Handling**: `.slx` (Simulink) support improved with larger per-file content limits (50 KB per XML/MDL file, 100 MB safety cap) and ZIP-based extraction

### Recent Improvements (Post-v2.1.8)
- **Tiny File Archiving**: Files under 100 bytes are automatically parked in `03_archive/skipped_files/` with their manifests to eliminate endless â€œtoo smallâ€ retries. - **Manifest & Hash Safety**: Watcher now skips any file containing `.origin.json` in its name and recomputes content hashes when the manifest is missing a checksum so incremental tracking remains intact. - **Chunk Writer Hardening**: Consolidated `write_chunk_files()` helper creates the directory once, writes UTF-8 chunks with defensive logging, and `copy_manifest_sidecar()` guarantees parent folders exist before copying manifests. - **Parallel Queue Handling**: Added optional `multiprocessing.Pool` batches for queues â‰¥32 files (config flag), plus automatic pruning of the `processed_files` set to prevent long-running watcher stalls. - **Tokenizer & Metrics Optimizations**: Sentence tokenization is LRU-cached, system metrics run on a background executor, and notification bursts are throttled with a 60-second rate limiter per alert key. - **SQLite Resilience**: Centralized `_conn()` helper sets 60â€¯s timeouts, `log_error()` now understands both legacy signatures and retries lock errors, and `run_integrity_check()` validates the DB at startup. - **Test Coverage & Pytest Guardrails**: Root `conftest.py` skips bulky `99_doc/legacy` suites and `tests/test_db.py` smoke-tests the new retry path to ensure future regressions fail fast. - **Database Lock Monitoring**: `MONITOR_DB_LOCKS.md` documents command-line checks, baseline metrics (1.5 errors/min), and alert thresholds (3 errors/min = 2Ã— baseline). - **Watcher Bridge Support**: `watcher_splitter.py` understands `.part` staging files, waits for optional `.ready` signals, retries processing up to three times, and quarantines stubborn failures to `03_archive/failed/`. - **Batched Chroma Ingest**: `ChromaRAG.add_chunks_bulk()` honours `batch.size`, skips null embeddings, and refreshes `hnsw:search_ef` from `config.json` so the vector store keeps pace with high-volume ingest. ### v2.1.8 Release (2025-11-07)
- **ChromaDB Rebuild**: Upgraded to `chromadb 1.3.4`, recreated the collection, and re-ran the backfill so 2,907 enriched chunks are in sync with the latest pipeline. - **Dedup Reliability**: `deduplication.py` now ships with `hnswlib` compatibility shims, letting `python deduplication.py --auto-remove` complete without legacy metadata errors. - **Release Helper**: `scripts/release_commit_and_tag.bat` automates doc staging, backups, commit/tag creation, and pushes while rotating logs; the 2025-11-07 dry run and live validation are logged in `docs/RELEASE_WORKFLOW.md`. - **Regression Tests**: Replaced placeholder suites with 52-case pytest coverage for query caching, incremental updates, backup management, and monitoring to mirror the production APIs. - **Watcher & DB Resilience (Novâ€¯2025)**: Skips manifests/archives/output files, sanitises output folder names, replaces Unicode logging arrows, adds safe archive moves, and introduces exponential-backoff SQLite retries to squash recursion, path-length, and â€œdatabase lockedâ€ errors. > **What changed in v2.1.8? ** See the [changelog entry](./CHANGELOG.md#v218---2025-11-07---chromadb-rebuild--release-automation). ## ğŸš€ What's New in v2.1.6

### ğŸš€ RAG Backfill Optimization
- **Multiprocessing**: Parallel file processing and ChromaDB inserts with 4-8 workers
- **Performance**: 20x faster backfill (100-200 chunks/second vs 5 chunks/second)
- **Batch Optimization**: Optimized batch sizes (500-1000 chunks) for ChromaDB efficiency
- **HNSW Tuning**: Proper vector index configuration (M=32, ef_construction=512, ef_search=200)
- **CPU Monitoring**: Real-time CPU tracking with saturation alerts
- **Duplicate Detection**: Pre-insertion verification prevents duplicate chunks
- **Verification Tools**: Comprehensive scripts to verify backfill completeness

### ğŸ“Š Verification & Validation
- **Empty Folder Logging**: Identifies folders without chunk files
- **Count Discrepancy Alerts**: Warns when expected vs actual counts differ
- **Chunk Completeness Verification**: Validates all chunks from all folders are in KB
- **Performance Metrics**: Detailed throughput, memory, and CPU statistics

---

## ğŸš€ What's New in v2.1.5

### ğŸ“¦ Move-Based Workflow (Grok Recommendations)
- **âš¡ Storage Optimization**: Reduced storage overhead by 50-60% via MOVE operations instead of COPY
- **ğŸ”— OneDrive Sync Elimination**: 100% reduction in sync overhead by moving files out of OneDrive
- **ğŸ“‹ Manifest Tracking**: Complete origin tracking with `.origin.json` files
- **ğŸ”„ Enhanced Archive**: MOVE with 3 retry attempts and graceful fallback to COPY
- **ğŸ¯ Department Organization**: Automatic folder structure in archive by department
- **ğŸ” Smart Retry Logic**: Handles Windows permission issues with automatic retries

---

## ğŸš€ What's New in v2.1.2

### ğŸš¨ Critical Performance Fixes
- **âš¡ Processing Loop Resolution**: Fixed infinite loops that caused system hangs
- **ğŸ“ Smart File Archiving**: Failed files automatically moved to organized archive folders
- **ğŸ”’ Database Stability**: Eliminated "database is locked" errors with batch operations
- **âš¡ 8-12x Speed Improvement**: Dynamic parallel workers and optimized processing

### ğŸš€ Performance Enhancements
- **ğŸ” Advanced RAG System**: Ollama + FAISS for local embeddings and semantic search
- **ğŸ“Š Comprehensive Evaluation**: Precision@K, Recall@K, MRR, ROUGE, BLEU, Faithfulness scoring
- **ğŸ”— LangSmith Integration**: Tracing, evaluation, and feedback collection
- **âš¡ Real-time Monitoring**: Watchdog-based file system monitoring with debouncing
- **ğŸ¤– Hybrid Search**: Combines semantic similarity with keyword matching
- **ğŸ“ˆ Automated Evaluation**: Scheduled testing with regression detection
- **ğŸ›¡ï¸ Production Ready**: Graceful degradation, error handling, and monitoring
- **ğŸ“‚ Source Folder Copying**: Configurable copying of processed files back to source locations

## Directory Structure

- **C:/_chunker** - Main project directory with scripts
- **02_data/** - Input files to be processed (watch folder)
- **03_archive/** - Archived original files and processed source files
- **03_archive/skipped_files/** - Files too small to process (< 100 bytes) - automatically archived
- **04_output/** - Generated chunks and transcripts (organized by source file)
- **05_logs/** - Application logs and tracking
- **06_config/** - Configuration files
- **99_doc/legacy/** - Consolidated legacy docs (latest snapshot per project)
- **06_config/legacy/** - Consolidated legacy config (latest snapshot per project)
- **05_logs/legacy/** - Consolidated legacy logs (latest snapshot per project)
- **03_archive/legacy/** - Consolidated legacy db/backups (latest snapshot per project)
- **chroma_db/** - ChromaDB vector database storage
- **faiss_index/** - FAISS vector database storage
- **evaluations/** - RAG evaluation results
- **reports/** - Automated evaluation reports

## ğŸš€ Quick Start

### Basic Usage (Core Chunking)
1. Place files to process in `02_data/` folder
2. Run the watcher: `python watcher_splitter.py`
3. Check `04_output/` for processed chunks and transcripts
4. Original files are moved to `03_archive/` after processing

### Advanced Usage (RAG-Enabled)
1. Install RAG dependencies: `python install_rag_dependencies.py`
2. Install Ollama and pull model: `ollama pull nomic-embed-text`
3. Enable RAG in `config.json`: Set `"rag_enabled": true`
4. Run the watcher: `python watcher_splitter.py`
5. Search knowledge base: `python rag_search.py`

### Advanced Usage (Celery-Enabled)
For high-volume processing and advanced task management:

1. **Install Celery Dependencies**:
   ```bash
   pip install celery redis flower
   ```

2. **Start Redis Server**:
   ```bash
   # Windows: Download from https://github.com/microsoftarchive/redis/releases
   redis-server
   
   # Linux: sudo apt-get install redis-server
   # macOS: brew install redis
   ```

3. **Start Celery Services**:
   ```bash
   # Option A: Use orchestrator (recommended)
   python orchestrator.py
   
   # Option B: Start manually
   celery -A celery_tasks worker --loglevel=info --concurrency=4
   celery -A celery_tasks beat --loglevel=info
   celery -A celery_tasks flower --port=5555
   python enhanced_watchdog.py
   ```

4. **Monitor Tasks**:
   - Flower Dashboard: http://localhost:5555 (with authentication)
   - Celery CLI: `celery -A celery_tasks inspect active`
   - Logs: Check `logs/watcher.log`

5. **Security & Priority Features**:
   - **Flower Authentication**: Default credentials logged on startup
   - **Priority Queues**: High-priority processing for legal/police files
   - **Redis Fallback**: Automatic fallback to direct processing if Redis fails
   - **Task Timeouts**: 300s hard limit with graceful handling

6. **Configuration**:
   ```json
   {
     "celery_enabled": true,
     "celery_broker": "redis://localhost:6379/0",
     "celery_task_time_limit": 300,
     "celery_worker_concurrency": 4,
     "priority_departments": ["legal", "police"]
   }
   ```

7. **Environment Variables** (Optional):
   ```bash
   export FLOWER_USERNAME="your_username"
   export FLOWER_PASSWORD="your_secure_password"
   ```

## âš™ï¸ Feature Toggles & Setup

All new subsystems ship **disabled by default** so existing deployments behave exactly as before. Enable individual features by updating `config.json` in the project root. ### Metadata Enrichment (`metadata_enrichment`)
- Adds semantic tags, key terms, summaries, and source metadata to chunk sidecars and manifests. - Output schema documented in `docs/METADATA_SCHEMA.md`. - Enable with:
  ```json
  "metadata_enrichment": { "enabled": true }
  ```

### Monitoring & Health Checks (`monitoring`)
- Background thread performs disk, throughput, and ChromaDB checks and escalates via the notification system. - Configure thresholds in `config.json` under the `monitoring` section; default recipients come from `notification_system.py`. - Start by setting `"monitoring": { "enabled": true }`. ### Database Lock Monitoring
- **Current Performance**: 1.5 database lock errors/minute baseline (68% reduction from previous)
- **Monitoring Documentation**: See `MONITOR_DB_LOCKS.md` for comprehensive monitoring commands and alert thresholds
- **Real-time Monitoring**:
  ```bash
  # Watch for lock errors in real-time
  powershell -Command "Get-Content watcher_live.log -Wait | Select-String -Pattern 'Failed to log|database is locked'"

  # Check hourly error count
  powershell -Command "(Get-Content watcher_live.log | Select-String -Pattern 'Failed to log processing' | Select-Object -Last 100 | Measure-Object).Count"
  ```
- **Alert Threshold**: Flag if errors exceed 3/minute (2x current baseline)
- **Review Schedule**: Monitor every 8-12 hours using commands in `MONITOR_DB_LOCKS.md`
- **Key Findings**:
  - 92% of lock errors occur in `log_processing()` (lacks retry wrapper)
  - 8% in `_update_department_stats()` (has 5-retry exponential backoff)
  - Current retry config: get_connection (3 retries), dept_stats (5 retries with 1.5x backoff)

### Tiny File Handling
- **Automatic Archiving**: Files under 100 bytes (default `min_file_size_bytes`) are automatically moved to `03_archive/skipped_files/`
- **Examples**: Empty files, "No measures found" messages, test placeholders
- **Behavior**: Files are preserved with their `.origin.json` manifests for review rather than deleted or left to trigger repeated warnings
- **Configuration**: Adjust threshold in department config via `min_file_size_bytes` parameter (default: 100)
- **Logs**: Look for `[INFO] File too short (X chars), archiving: filename` messages

### Deduplication (`deduplication`)
- Prevents duplicate chunks from entering ChromaDB in both watcher and backfill flows. - Optionally run cleanup via `python deduplication.py --auto-remove`. - Already present in `config.json`; flip `"enabled": true` to activate. ### Query Cache (`query_cache`)
- Enables an in-memory LRU + TTL cache in `rag_integration.py` so repeat queries avoid hitting ChromaDB. - Configure `max_size`, `ttl_hours`, and optional `memory_limit_mb` under the `query_cache` section. - API users can inspect runtime metrics via `GET /api/cache/stats` once the cache is enabled. - Minimal example:
  ```json
  "query_cache": {
    "enabled": true,
    "max_size": 256,
    "ttl_hours": 1,
    "memory_limit_mb": 64
  }
  ```

### Incremental Updates (`incremental_updates`)
- Uses a shared `VersionTracker` to hash inputs, skip untouched files, remove old chunk IDs, and persist deterministic chunk identifiers. - Tracker state defaults to `06_config/file_versions.json` (override with `version_file`). - Typical configuration:
  ```json
  "incremental_updates": {
    "enabled": true,
    "version_file": "06_config/file_versions.json",
    "hash_algorithm": "sha256"
  }
  ```
- After enabling, unchanged files are skipped by the watcher/backfill, while reprocessed sources clean up stale artifacts before writing new chunks. ### Backup Manager (`backup`)
- Creates compressed archives of ChromaDB and critical directories on a schedule. - Configure destination, retention, and schedule in the `backup` section. - Manual run: `python backup_manager.py --config config.json create --label on-demand`. After toggling features, restart the watcher (`python watcher_splitter.py`) so runtime components reinitialize with the new configuration. ## âœ¨ Features

### Core Chunking
- [x] **Organized output** by source file name with timestamp prefixes
- [x] **Multi-file type support** - .txt, .md, .json, .csv, .xlsx, .pdf, .py, .docx, .sql, .yaml, .xml, .log
- [x] **Unicode filename support** - Handles files with emojis, special characters, and symbols
- [x] **Enhanced filename sanitization** - Automatically cleans problematic characters
- [x] **Database tracking and logging** - Comprehensive activity monitoring
- [x] **Automatic file organization** - Moves processed files to archive

### RAG System (v2.0)
- [x] **Ollama Integration** - Local embeddings with nomic-embed-text model
- [x] **FAISS Vector Database** - High-performance similarity search
- [x] **Hybrid Search** - Combines semantic similarity with keyword matching
- [x] **ChromaDB Support** - Alternative vector database (optional)
- [x] **Real-time Monitoring** - Watchdog-based file system monitoring
- [x] **Debounced Processing** - Prevents race conditions and duplicate processing

### Performance & Scalability (v2.1.2+)
- [x] **Dynamic Parallel Processing** - Up to 12 workers for large batches (50+ files)
- [x] **Batch Processing** - Configurable batch sizes with system overload protection (default 100 files per cycle)
- [x] **Stability Skip Optimization** - Files >10 minutes old bypass stability checks (configurable via `stability_skip_minutes`)
- [x] **Database Optimization** - Batch logging eliminates locking issues
- [x] **Smart File Archiving** - Failed files automatically moved to organized folders
- [x] **Real-time Performance Metrics** - Files/minute, avg processing time, peak CPU/memory
- [x] **Large Backlog Support** - Handles 5,000+ file backlogs efficiently (3.5 hours â†’ 53 minutes for 6,500 files)
- [x] **Source Folder Copying** - Configurable copying of processed files back to source locations
- [x] **Multiprocessing Support** - Optional process pool for CPU-bound workloads with automatic fallback

### Evaluation & Quality Assurance
- [x] **Comprehensive Metrics** - Precision@K, Recall@K, MRR, NDCG@K
- [x] **Generation Quality** - ROUGE-1/2/L, BLEU, BERTScore
- [x] **Faithfulness Scoring** - Evaluates answer grounding in source context
- [x] **Context Utilization** - Measures how much context is used in answers
- [x] **Automated Evaluation** - Scheduled testing with regression detection
- [x] **LangSmith Integration** - Tracing, evaluation, and feedback collection

### Production Features
- [x] **Graceful Degradation** - Continues working even if RAG components fail
- [x] **Error Handling** - Robust error recovery and logging
- [x] **Performance Monitoring** - System metrics and performance tracking
- [x] **Security Redaction** - PII masking in metadata
- [x] **Modular Architecture** - Clean separation of concerns
- [x] **JSON Sidecar (optional)** - Per-file sidecar with chunk list, metadata, and Python code blocks

### Windows "Send to" & Directory Opus Integration
**Send To Menu (Recommended):**
1. Batch file installed at: `C:\Users\carucci_r\AppData\Roaming\Microsoft\Windows\SendTo\Chunker_MoveOptimized.bat`
2. Rightâ€‘click any file â†’ Send to â†’ Chunker Move-Optimized
3. Files are moved to `02_data` with `.origin.json` manifest files
4. Watcher automatically processes files from `02_data`

**Directory Opus Button:**
1.